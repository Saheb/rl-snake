{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udc0d Reinforcement Learning for Snake: Zero to Hero\n",
        "\n",
        "Welcome to this interactive tutorial on training an AI agent to play Snake using Reinforcement Learning (RL). This notebook aligns with our project blog post: **Scaling Snake AI: From Random Wiggles to Strategic Mastery**.\n",
        "\n",
        "## 0. A Quick Primer: What is RL?\n",
        "Reinforcement Learning is simply **learning by trial and error**. Imagine training a dog: you don't give it a manual on how to sit; you give it a treat when it accidentally sits. Eventually, the dog learns that **Actions** in certain **States** lead to **Rewards**.\n",
        "\n",
        "### The Two Main 'Brains'\n",
        "There are two primary ways an AI can 'think' about this problem:\n",
        "\n",
        "**1. Value-Based (The Accountant)**\n",
        "- *Method*: The agent tries to calculate the exact worth of every move. \"Is turning left worth 10 points or 2?\"\n",
        "- *The Famous Name*: **DQN** (stands for **Deep Q-Network**). It's 'Deep' because it uses a neural network brain, and 'Q' is just the math symbol for 'Quality' or value.\n",
        "- *Analogy*: It's like having a map of a city that tells you exactly how much gold is at every corner.\n",
        "\n",
        "**2. Policy-Based (The Athlete)**\n",
        "- *Method*: The agent learns general instincts. \"If a wall is in front of me, turn right.\" It doesn't calculate value; it just knows the right response.\n",
        "- *The Famous Name*: **PPO** (stands for **Proximal Policy Optimization**). 'Proximal' basically means 'don't change too much at once' so it stays stable while learning.\n",
        "- *Analogy*: It's like a professional athlete. They don't calculate the 'value' of a pass; they have a trained instinct that tells them where to throw the ball.\n",
        "\n",
        "---\n",
        "## 1. The Environment\n",
        "\n",
        "We use a custom `SnakeGame` environment. The agent observes the state and outputs one of 3 actions: [Straight, Right Turn, Left Turn]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from snake_game import SnakeGame\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "game = SnakeGame(board_size=5)\n",
        "state = game.reset()\n",
        "print(f\"Initial Board State (Shape {state.shape}):\")\n",
        "print(state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Phase 0: The Baseline (Value-Based)\n",
        "\n",
        "We started with **Classic Tabular Q-Learning**. This is the ultimate 'Accountant' approach, where we store a literal table of every possible state.\n",
        "\n",
        "- **Tabular Q**: Simple memorization.\n",
        "- **Double Q**: A smarter version that double-checks its own math to avoid over-confidence.\n",
        "\n",
        "**Result**: Perfect on small 5x5 boards, but failing as soon as the grid grew because the \"table\" became too large to fit in memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try running the tabular agents:\n",
        "# !python train_tabular_q.py --type double_q --board_size 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. The Wall: Scaling Challenges\n",
        "\n",
        "On a 10x10 board, calculations fail. Rewards become **sparse**: the snake may wander for 1000 steps without seeing a single piece of food. Random exploration is no longer enough.\n",
        "\n",
        "## 4. The Solution: A Triple Threat\n",
        "\n",
        "To master the 10x10 board, we combined three advanced techniques:\n",
        "\n",
        "### 1. Imitation Learning (The Instinct)\n",
        "We taught the agent to mimic experts. This gave it an \"instinct\" for survival immediately.\n",
        "\n",
        "### 2. PPO (Proximal Policy Optimization)\n",
        "Because PPO learns high-level strategies (\"move toward food\") rather than specific board values, it generalizes much better to different board sizes.\n",
        "\n",
        "### 3. Curriculum Learning (The Growth)\n",
        "We didn't start at 10x10. We mastered 5x5, then transferred that brain to 8x8, and finally to 10x10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "print(\"To see the full scaling pipeline, check train_ppo_curriculum.py\")\n",
        "# !python train_ppo_curriculum.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualizing the Journey\n",
        "\n",
        "Check out `snake_learning_journey.html` to see the interactive evolution from Stage 0 (Classic Tabular) to Stage 8 (Master PPO)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}