{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üêç Reinforcement Learning for Snake: Zero to Hero\n",
        "\n",
        "Welcome! This notebook is a **hands-on, runnable** tutorial on training an AI agent to play Snake.\n",
        "We'll go from a random agent to a scaled-up PPO master ‚Äî and you can run every cell.\n",
        "\n",
        "> **Companion material**: Read [blog.md](blog.md) for the full narrative, or watch the [Live Visualization](https://Saheb.github.io/rl-snake/snake_learning_journey.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 0. A Quick Primer: What is Reinforcement Learning?\n",
        "\n",
        "RL is **learning by trial and error**. An *agent* takes *actions* in an *environment*,\n",
        "and receives *rewards* (or penalties). Over time, it learns a *policy* ‚Äî a strategy\n",
        "that maximises its cumulative reward.\n",
        "\n",
        "```\n",
        "   Agent ‚îÄ‚îÄaction‚îÄ‚îÄ‚ñ∂ Environment\n",
        "     ‚ñ≤                  ‚îÇ\n",
        "     ‚îî‚îÄ‚îÄstate, reward‚óÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "### Two Families of RL\n",
        "\n",
        "| | Value-Based (\"The Accountant\") | Policy-Based (\"The Athlete\") |\n",
        "|---|---|---|\n",
        "| **How it thinks** | Calculates the *worth* of every move | Learns *instincts* directly |\n",
        "| **Famous algorithm** | **DQN** (Deep Q-Network) | **PPO** (Proximal Policy Optimization) |\n",
        "| **Analogy** | A map that tells you the gold at every corner | An athlete who just *knows* where to throw the ball |\n",
        "| **Scales to big boards?** | ‚ùå Table explodes | ‚úÖ Generalises well |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Meet the Environment\n",
        "\n",
        "Our game is a custom `SnakeGame` class. Let's create one and see what the agent \"sees\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from snake_game import SnakeGame, GameState, EMPTY, SNAKE, FOOD\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "game = SnakeGame(board_size=5, seed=42)\n",
        "state = game.reset(seed=42)\n",
        "\n",
        "print(f\"Board size : {game.board_size}x{game.board_size}\")\n",
        "print(f\"Actions    : 0=Up, 1=Right, 2=Down, 3=Left\")\n",
        "print(f\"Snake pos  : {list(game.snake_position)}\")\n",
        "print(f\"Food pos   : {game.food_position}\")\n",
        "print()\n",
        "game.print_board()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's **visualise** a board as a colour grid so we can see what's happening:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_board(game, title=\"Snake Game\"):\n",
        "    \"\"\"Render the board as a colour image.\"\"\"\n",
        "    cmap = mcolors.ListedColormap(['#1a1a2e', '#16c784', '#ff6b6b'])\n",
        "    bounds = [-0.5, 0.5, 1.5, 2.5]\n",
        "    norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(4, 4))\n",
        "    ax.imshow(game.board, cmap=cmap, norm=norm)\n",
        "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks([]); ax.set_yticks([])\n",
        "    for i in range(game.board_size + 1):\n",
        "        ax.axhline(i - 0.5, color='white', linewidth=0.5, alpha=0.3)\n",
        "        ax.axvline(i - 0.5, color='white', linewidth=0.5, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_board(game, \"Initial Board (üü¢ Snake, üî¥ Food)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. A Random Agent (The Baseline)\n",
        "\n",
        "Before any learning, let's see how a **random agent** performs.\n",
        "This is our \"Phase ?\" ‚Äî the absolute floor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def run_random_agent(board_size=5, num_games=500, max_steps=200):\n",
        "    \"\"\"Run a random agent and collect scores.\"\"\"\n",
        "    game = SnakeGame(board_size=board_size)\n",
        "    scores = []\n",
        "    for _ in range(num_games):\n",
        "        game.reset()\n",
        "        for _ in range(max_steps):\n",
        "            action = random.randint(0, 3)\n",
        "            _, _, done, info = game.step(action)\n",
        "            if done:\n",
        "                break\n",
        "        scores.append(info['score'])\n",
        "    return scores\n",
        "\n",
        "random_scores = run_random_agent()\n",
        "print(f\"Random Agent on 5x5:\")\n",
        "print(f\"  Mean Score : {np.mean(random_scores):.2f}\")\n",
        "print(f\"  Max Score  : {max(random_scores)}\")\n",
        "print(f\"  Median     : {np.median(random_scores):.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected ‚Äî the random agent barely scores. It usually dies in a few steps.\n",
        "\n",
        "---\n",
        "## 3. Phase 0: Tabular Q-Learning (The Accountant)\n",
        "\n",
        "Our first real algorithm. The agent maintains a **Q-table** ‚Äî a dictionary that maps\n",
        "`(state, action)` ‚Üí expected future reward. After every step, it updates the table:\n",
        "\n",
        "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\Big[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\Big]$$\n",
        "\n",
        "Let's train one **live** and watch the Q-table grow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from train_tabular_q import QLearningAgent, DoubleQLearningAgent\n",
        "\n",
        "# Train a fresh Q-Learning agent (small run for demo)\n",
        "game = SnakeGame(board_size=5)\n",
        "agent = QLearningAgent(learning_rate=0.1, discount_factor=0.99, epsilon=1.0)\n",
        "\n",
        "scores = []\n",
        "q_table_sizes = []  # Track how many states the agent discovers\n",
        "\n",
        "for ep in range(2000):\n",
        "    game.reset()\n",
        "    state = agent.get_state_key(game, 0)\n",
        "    action = agent.choose_action(state, game)\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        _, reward, done, info = game.step(action)\n",
        "        next_state = agent.get_state_key(game, action)\n",
        "        agent.update_q_value(state, action, reward, next_state)\n",
        "        state = next_state\n",
        "        action = agent.choose_action(next_state, game)\n",
        "\n",
        "    scores.append(info['score'])\n",
        "    q_table_sizes.append(len(agent.q_table))\n",
        "    agent.epsilon = max(0.01, agent.epsilon * 0.995)\n",
        "\n",
        "print(f\"Training complete! Q-table has {len(agent.q_table):,} unique states.\")\n",
        "print(f\"Last 100 episodes ‚Äî Mean: {np.mean(scores[-100:]):.1f}, Max: {max(scores[-100:])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot: Training Curve + Q-table Growth (side by side)\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Smoothed training curve\n",
        "window = 50\n",
        "smoothed = np.convolve(scores, np.ones(window)/window, mode='valid')\n",
        "ax1.plot(smoothed, color='#16c784', linewidth=1.5)\n",
        "ax1.set_title('Training Curve (Q-Learning)', fontweight='bold')\n",
        "ax1.set_xlabel('Episode')\n",
        "ax1.set_ylabel('Score')\n",
        "ax1.set_facecolor('#1a1a2e')\n",
        "ax1.grid(alpha=0.2)\n",
        "\n",
        "# Q-table growth\n",
        "ax2.plot(q_table_sizes, color='#ff6b6b', linewidth=1.5)\n",
        "ax2.set_title('Q-Table Size (States Discovered)', fontweight='bold')\n",
        "ax2.set_xlabel('Episode')\n",
        "ax2.set_ylabel('Unique States')\n",
        "ax2.set_facecolor('#1a1a2e')\n",
        "ax2.grid(alpha=0.2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"üëÜ Notice: The Q-table keeps growing. On a 10x10 board, this would explode!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Phase 0+: Double Q-Learning\n",
        "\n",
        "Standard Q-Learning tends to **overestimate** values. Double Q-Learning fixes this by\n",
        "maintaining *two* tables and cross-checking:\n",
        "\n",
        "- Table A picks the best action\n",
        "- Table B evaluates it (and vice versa)\n",
        "\n",
        "This gives more **stable** and **reliable** learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the pre-trained models and compare\n",
        "import pickle\n",
        "\n",
        "def evaluate_agent(agent, board_size=5, num_games=200, max_steps=500):\n",
        "    \"\"\"Evaluate a trained agent.\"\"\"\n",
        "    game = SnakeGame(board_size=board_size)\n",
        "    scores = []\n",
        "    for _ in range(num_games):\n",
        "        game.reset()\n",
        "        action = 0\n",
        "        for _ in range(max_steps):\n",
        "            state_key = agent.get_state_key(game, action)\n",
        "            old_eps = agent.epsilon\n",
        "            agent.epsilon = 0  # Greedy during eval\n",
        "            action = agent.choose_action(state_key, game)\n",
        "            agent.epsilon = old_eps\n",
        "            _, _, done, info = game.step(action)\n",
        "            if done:\n",
        "                break\n",
        "        scores.append(info['score'])\n",
        "    return scores\n",
        "\n",
        "try:\n",
        "    with open('tabular_q_5x5.pkl', 'rb') as f:\n",
        "        q_agent = pickle.load(f)\n",
        "    with open('tabular_double_q_5x5.pkl', 'rb') as f:\n",
        "        dq_agent = pickle.load(f)\n",
        "\n",
        "    q_scores = evaluate_agent(q_agent)\n",
        "    dq_scores = evaluate_agent(dq_agent)\n",
        "\n",
        "    print(\"=\" * 40)\n",
        "    print(f\"{'Metric':<20} {'Q-Learning':>10} {'Double Q':>10}\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"{'Mean Score':<20} {np.mean(q_scores):>10.1f} {np.mean(dq_scores):>10.1f}\")\n",
        "    print(f\"{'Max Score':<20} {max(q_scores):>10} {max(dq_scores):>10}\")\n",
        "    print(f\"{'Std Dev':<20} {np.std(q_scores):>10.1f} {np.std(dq_scores):>10.1f}\")\n",
        "    print(f\"{'Q-Table States':<20} {len(q_agent.q_table):>10,} {len(dq_agent.q_table_a):>10,}\")\n",
        "    print(\"=\" * 40)\n",
        "except FileNotFoundError:\n",
        "    print(\"Pre-trained models not found. Run `python train_tabular_q.py` first!\")\n",
        "    print(\"  python train_tabular_q.py --type q --episodes 5000\")\n",
        "    print(\"  python train_tabular_q.py --type double_q --episodes 5000\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. üöß The Wall: Why Tables Don't Scale\n",
        "\n",
        "On a 5x5 board, tabular Q-Learning is perfect. But try a **10x10** board:\n",
        "\n",
        "| Board | Possible States | Can Tabular Handle It? |\n",
        "|---|---|---|\n",
        "| 5√ó5 | ~2,000 | ‚úÖ Easy |\n",
        "| 8√ó8 | ~50,000+ | ‚ö†Ô∏è Barely |\n",
        "| 10√ó10 | ~500,000+ | ‚ùå Impossible |\n",
        "\n",
        "The Q-table can't memorise that many states. And even if it could,\n",
        "**rewards are sparse**: a random snake on a 10x10 board might wander for 1000 steps\n",
        "before accidentally eating food. There's nothing to learn from.\n",
        "\n",
        "We need **neural networks** to *generalise* across similar states,\n",
        "and **clever training strategies** to overcome the sparse reward problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. The Solution: A Triple Threat\n",
        "\n",
        "To conquer the 10x10 board, we combined three techniques:\n",
        "\n",
        "### üß† Technique 1: PPO (Proximal Policy Optimization)\n",
        "Instead of memorising a table, PPO uses a **neural network** that outputs\n",
        "action *probabilities*. It's a policy-gradient method, meaning it directly\n",
        "optimises \"what action should I take?\" rather than \"what is this state worth?\"\n",
        "\n",
        "The key idea: PPO clips its updates so it never changes the policy too drastically\n",
        "in one step. This gives stable, reliable learning.\n",
        "\n",
        "### üéì Technique 2: Imitation Learning (Behavioral Cloning)\n",
        "Before exploring on its own, we let the agent **watch an expert** play.\n",
        "The expert is a REINFORCE agent trained on 5x5. We record its games and\n",
        "pre-train the PPO network to mimic those moves. This gives the agent\n",
        "\"instincts\" before it even starts exploring.\n",
        "\n",
        "### üìà Technique 3: Curriculum Learning\n",
        "We don't jump straight to 10x10. Instead:\n",
        "1. Master **5x5** (easy, dense rewards)\n",
        "2. Transfer brain ‚Üí **8x8** (medium)\n",
        "3. Transfer brain ‚Üí **10x10** (the goal)\n",
        "\n",
        "Each stage builds on the skills from the previous one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The PPO Network Architecture\n",
        "\n",
        "Let's peek inside the neural network that powers our final agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    \"\"\"The brain of our PPO agent.\n",
        "    \n",
        "    - Actor: outputs action probabilities (\"what should I do?\")\n",
        "    - Critic: estimates state value (\"how good is my situation?\")\n",
        "    - Shared layers: both heads share a common feature extractor\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.actor = nn.Linear(hidden_size // 2, output_size)   # ‚Üí 3 actions\n",
        "        self.critic = nn.Linear(hidden_size // 2, 1)            # ‚Üí 1 value\n",
        "    \n",
        "    def forward(self, x):\n",
        "        shared = self.shared(x)\n",
        "        return F.softmax(self.actor(shared), dim=-1), self.critic(shared)\n",
        "\n",
        "# Create the network\n",
        "net = ActorCritic(input_size=14, hidden_size=256, output_size=3)\n",
        "print(net)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in net.parameters()):,}\")\n",
        "\n",
        "# Test with a dummy state\n",
        "dummy = torch.randn(1, 14)\n",
        "probs, value = net(dummy)\n",
        "print(f\"\\nAction probabilities: {probs.detach().numpy().round(3)}\")\n",
        "print(f\"State value estimate: {value.item():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice the architecture:\n",
        "- **14 input features** (danger sensors, food direction, movement direction, etc.)\n",
        "- **256 ‚Üí 128** shared hidden layers (the \"common brain\")\n",
        "- **Actor head** ‚Üí 3 outputs (Straight / Turn Right / Turn Left)\n",
        "- **Critic head** ‚Üí 1 output (\"how good is this state?\")\n",
        "\n",
        "This is *much* more compact than a Q-table with thousands of entries!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. What *Failed* (And Why It Matters)\n",
        "\n",
        "Not everything worked. Here's what we tried and what we learned:\n",
        "\n",
        "| Approach | Board | Result | Why It Failed |\n",
        "|---|---|---|---|\n",
        "| **A2C** (Actor-Critic) | 5x5 | Max 4, Mean 0.2 | Bootstrap trap: critic gives bad estimates early, actor trusts them anyway |\n",
        "| **Vanilla PPO** | 5x5 | Max 4, Mean 0.35 | Same bootstrap trap ‚Äî no expert guidance |\n",
        "| **PPO + DQN demos** | 8x8 | Max 42, Mean 6.4 | DQN demos had 96% accuracy but PPO couldn't replicate the value-based style |\n",
        "| **PPO + REINFORCE demos** | 8x8 | Max 46, Mean 11.9 | ‚úÖ Policy‚ÜíPolicy transfer works! |\n",
        "\n",
        "### üí° Key Insight: Algorithm Compatibility > Imitation Accuracy\n",
        "\n",
        "DQN demos gave **96% behavioral cloning accuracy** (higher than REINFORCE's 82%),\n",
        "but PPO performed *worse* with them! Why?\n",
        "\n",
        "- **REINFORCE and PPO are both policy-gradient methods** ‚Äî they \"think\" the same way\n",
        "- **DQN is value-based** ‚Äî it outputs Q-values, not probabilities\n",
        "- Transferring from a policy method ‚Üí policy method preserves the decision structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Existing Training Plots\n",
        "\n",
        "Here are the training curves from our full runs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "plots = ['tabular_q_training.png', 'tabular_double_q_training.png']\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "for ax, plot_file in zip(axes, plots):\n",
        "    if os.path.exists(plot_file):\n",
        "        img = plt.imread(plot_file)\n",
        "        ax.imshow(img)\n",
        "        ax.set_title(plot_file.replace('.png', '').replace('_', ' ').title(), fontweight='bold')\n",
        "    else:\n",
        "        ax.text(0.5, 0.5, f'{plot_file}\\nnot found', ha='center', va='center')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle('Full Training Runs (5000 episodes each)', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. üèÜ Final Results\n",
        "\n",
        "| Phase | Strategy | Board | Best Score | Mean Score |\n",
        "|---|---|---|---|---|\n",
        "| 0 | Random Agent | 5x5 | ~2 | ~0.3 |\n",
        "| 0 | Tabular Q-Learning | 5x5 | 24 (Perfect) | ~11 |\n",
        "| 0+ | Double Q-Learning | 5x5 | 24 (Stable) | ~14 |\n",
        "| 1 | PPO + Imitation (5x5) | 5x5 | 24 | ~20 |\n",
        "| 2 | Curriculum ‚Üí 8x8 | 8x8 | 46 | ~12 |\n",
        "| **3** | **Curriculum ‚Üí 10x10** | **10x10** | **64** | **~18** |\n",
        "\n",
        "The agent fills over **60%** of a 10x10 board ‚Äî starting from literally nothing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 9. Try It Yourself!\n",
        "\n",
        "**Train your own agents:**\n",
        "```bash\n",
        "# Tabular (fast, runs in seconds)\n",
        "python train_tabular_q.py --type q --episodes 5000\n",
        "python train_tabular_q.py --type double_q --episodes 5000\n",
        "\n",
        "# Full PPO curriculum (takes ~30 min)\n",
        "python train_ppo_curriculum.py\n",
        "```\n",
        "\n",
        "**Watch the results:**\n",
        "- Open `snake_learning_journey.html` in your browser to see the full evolution\n",
        "- Open `snake_10x10_replay.html` to watch the best 10x10 games\n",
        "\n",
        "**Explore the code:**\n",
        "| File | What it does |\n",
        "|---|---|\n",
        "| `snake_game.py` | The game environment |\n",
        "| `train_tabular_q.py` | Q-Learning and Double Q-Learning |\n",
        "| `train_reinforce.py` | REINFORCE (expert for demos) |\n",
        "| `train_ppo_curriculum.py` | PPO + Imitation + Curriculum |\n",
        "| `visualize_journey.py` | Record games ‚Üí HTML visualization |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "*Happy learning! üêç*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}